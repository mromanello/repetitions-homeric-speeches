{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Passim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from lib.utils import *\n",
    "from lib.evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "| Experiment ID     | Passim parameters | Explanation  | \\# of extracted clusters | Evaluation |\n",
    "| ----------- | ----------- |----------|------------|-------------------|\n",
    "| `exp0`      | `-n 1 --min-match 1 -a 5` | reused passages consist of at least 1 shared n-grams of size 1 (uni-gram) | 6419 (lemmatised); 7993 (raw) | # of ground-truth cluster: 69, # of matched clusters: 22, # of unmatched clusters: 47, # of partially matched clusters: 2, # of exactly matched clusters: 2, # of clusters with spurious passages: 18|\n",
    "| `exp4`|`-n 2 --min-match 2 --max-repeat 100 -a 10`|reused passages consist of at least 2 shared n-grams of size 2 (bi-grams), and the aligned passage should be at least 10 characters long (default is `20`)|1909 (lemmatised), 2078 (raw)|# of ground-truth cluster: 69, # of matched clusters: 46, # of unmatched clusters: 23, # of partially matched clusters: 2, # of exactly matched clusters: 20, # of clusters with spurious passages: 24|\n",
    "| `exp5`|`-n 3 --min-match 1 --max-repeat 100 -a 10`|reused passages consist of at least 1 shared n-grams of size 3 (tri-grams), and the aligned passage should be at least 10 characters long (default is `20`)|350 (lemmatised), 431 (raw)|# of ground-truth cluster: 69, # of matched clusters: 58, # of unmatched clusters: 11, # of partially matched clusters: 3, # of exactly matched clusters: 44, # of clusters with spurious passages: 11|\n",
    "| `exp6`|`-n 4 --min-match 1 --max-repeat 100 -a 10`|reused passages consist of at least 1 shared n-grams of size 4 (bi-grams), and the aligned passage should be at least 10 characters long (default is `20`)|284 (lemmatised), 341 (raw)|# of ground-truth cluster: 69, # of matched clusters: 58, # of unmatched clusters: 11, # of partially matched clusters: 3, # of exactly matched clusters: 47, # of clusters with spurious passages: 8|\n",
    "| `exp7`|`-n 3 --min-match 2 --max-repeat 100 -a 10`|reused passages consist of at least 1 shared n-grams of size 3 (tri-grams), and the aligned passage should be at least 10 characters long (default is `20`)|350 (lemmatised), 431 (raw)|# of ground-truth cluster: 69, # of matched clusters: 58, # of unmatched clusters: 11, # of partially matched clusters: 3, # of exactly matched clusters: 44, # of clusters with spurious passages: 11|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 0 (exp0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "output_path = 'data/passim/exp0/'\n",
    "tsv_path = 'data/output/passim_clusters_exp0.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Applications/spark-3.3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/mromanel/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/mromanel/.ivy2/jars\n",
      "com.github.scopt#scopt_2.12 added as a dependency\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7157a7f0-389d-40b2-a698-ef7976e1438c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.github.scopt#scopt_2.12;3.5.0 in central\n",
      "\tfound graphframes#graphframes;0.8.0-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 169ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.scopt#scopt_2.12;3.5.0 from central in [default]\n",
      "\tgraphframes#graphframes;0.8.0-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7157a7f0-389d-40b2-a698-ef7976e1438c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "23/11/13 10:03:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/13 10:03:12 INFO SparkContext: Running Spark version 3.3.2\n",
      "23/11/13 10:03:12 INFO ResourceUtils: ==============================================================\n",
      "23/11/13 10:03:12 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/11/13 10:03:12 INFO ResourceUtils: ==============================================================\n",
      "23/11/13 10:03:12 INFO SparkContext: Submitted application: passim.PassimApp\n",
      "23/11/13 10:03:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/11/13 10:03:12 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/11/13 10:03:12 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/11/13 10:03:12 INFO SecurityManager: Changing view acls to: mromanel\n",
      "23/11/13 10:03:12 INFO SecurityManager: Changing modify acls to: mromanel\n",
      "23/11/13 10:03:12 INFO SecurityManager: Changing view acls groups to: \n",
      "23/11/13 10:03:12 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/11/13 10:03:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(mromanel); groups with view permissions: Set(); users  with modify permissions: Set(mromanel); groups with modify permissions: Set()\n",
      "23/11/13 10:03:12 INFO Utils: Successfully started service 'sparkDriver' on port 50395.\n",
      "23/11/13 10:03:12 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/11/13 10:03:12 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/11/13 10:03:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/11/13 10:03:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/11/13 10:03:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/11/13 10:03:12 INFO DiskBlockManager: Created local directory at /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/blockmgr-5d7f0027-8f67-4bdb-b61d-9f0539cc374e\n",
      "23/11/13 10:03:12 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/11/13 10:03:12 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/11/13 10:03:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/11/13 10:03:12 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/com.github.scopt_scopt_2.12-3.5.0.jar at spark://lettres-118-35.unil.ch:50395/jars/com.github.scopt_scopt_2.12-3.5.0.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:12 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar at spark://lettres-118-35.unil.ch:50395/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:12 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://lettres-118-35.unil.ch:50395/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:12 INFO SparkContext: Added JAR file:/Users/mromanel/Documents/passim-1.0.0/target/scala-2.12/passim_2.12-0.2.0.jar at spark://lettres-118-35.unil.ch:50395/jars/passim_2.12-0.2.0.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO Executor: Starting executor ID driver on host lettres-118-35.unil.ch\n",
      "23/11/13 10:03:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/11/13 10:03:13 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50395/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO TransportClientFactory: Successfully created connection to lettres-118-35.unil.ch/130.223.118.35:50395 after 30 ms (0 ms spent in bootstraps)\n",
      "23/11/13 10:03:13 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50395/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/fetchFileTemp16810074473699614266.tmp\n",
      "23/11/13 10:03:13 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar to class loader\n",
      "23/11/13 10:03:13 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50395/jars/passim_2.12-0.2.0.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50395/jars/passim_2.12-0.2.0.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/fetchFileTemp4393892570678245168.tmp\n",
      "23/11/13 10:03:13 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/passim_2.12-0.2.0.jar to class loader\n",
      "23/11/13 10:03:13 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50395/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50395/jars/org.slf4j_slf4j-api-1.7.16.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/fetchFileTemp10488466867369647312.tmp\n",
      "23/11/13 10:03:13 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
      "23/11/13 10:03:13 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50395/jars/com.github.scopt_scopt_2.12-3.5.0.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50395/jars/com.github.scopt_scopt_2.12-3.5.0.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/fetchFileTemp12587294255740282014.tmp\n",
      "23/11/13 10:03:13 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/com.github.scopt_scopt_2.12-3.5.0.jar to class loader\n",
      "23/11/13 10:03:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50397.\n",
      "23/11/13 10:03:13 INFO NettyBlockTransferService: Server created on lettres-118-35.unil.ch:50397\n",
      "23/11/13 10:03:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/11/13 10:03:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lettres-118-35.unil.ch, 50397, None)\n",
      "23/11/13 10:03:13 INFO BlockManagerMasterEndpoint: Registering block manager lettres-118-35.unil.ch:50397 with 434.4 MiB RAM, BlockManagerId(driver, lettres-118-35.unil.ch, 50397, None)\n",
      "23/11/13 10:03:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lettres-118-35.unil.ch, 50397, None)\n",
      "23/11/13 10:03:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, lettres-118-35.unil.ch, 50397, None)\n",
      "23/11/13 10:03:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/11/13 10:03:13 INFO SharedState: Warehouse path is 'file:/Users/mromanel/Documents/repetitions-homeric-speeches/spark-warehouse'.\n",
      "23/11/13 10:03:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/11/13 10:03:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/11/13 10:03:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/11/13 10:03:15 INFO CodeGenerator: Code generated in 157.403739 ms\n",
      "23/11/13 10:03:15 INFO SparkContext: Starting job: json at PassimApp.scala:1360\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Got job 0 (json at PassimApp.scala:1360) with 1 output partitions\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Final stage: ResultStage 0 (json at PassimApp.scala:1360)\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Submitting ResultStage 0 (CoalescedRDD[2] at json at PassimApp.scala:1360), which has no missing parents\n",
      "23/11/13 10:03:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 156.6 KiB, free 434.2 MiB)\n",
      "23/11/13 10:03:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 56.3 KiB, free 434.2 MiB)\n",
      "23/11/13 10:03:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on lettres-118-35.unil.ch:50397 (size: 56.3 KiB, free: 434.3 MiB)\n",
      "23/11/13 10:03:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "23/11/13 10:03:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (CoalescedRDD[2] at json at PassimApp.scala:1360) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/13 10:03:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/11/13 10:03:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (lettres-118-35.unil.ch, executor driver, partition 0, PROCESS_LOCAL, 5063 bytes) taskResourceAssignments Map()\n",
      "23/11/13 10:03:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/11/13 10:03:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/11/13 10:03:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/11/13 10:03:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311131003152978051997185313161_0000_m_000000_0' to file:/Users/mromanel/Documents/repetitions-homeric-speeches/data/passim/out_cluster/conf/_temporary/0/task_202311131003152978051997185313161_0000_m_000000\n",
      "23/11/13 10:03:16 INFO SparkHadoopMapRedUtil: attempt_202311131003152978051997185313161_0000_m_000000_0: Committed. Elapsed time: 1 ms.\n",
      "23/11/13 10:03:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1452 bytes result sent to driver\n",
      "23/11/13 10:03:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 215 ms on lettres-118-35.unil.ch (executor driver) (1/1)\n",
      "23/11/13 10:03:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/11/13 10:03:16 INFO DAGScheduler: ResultStage 0 (json at PassimApp.scala:1360) finished in 0.729 s\n",
      "23/11/13 10:03:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/13 10:03:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/11/13 10:03:16 INFO DAGScheduler: Job 0 finished: json at PassimApp.scala:1360, took 0.762568 s\n",
      "23/11/13 10:03:16 INFO FileFormatWriter: Start to commit write Job d14c1b2a-47cf-42c6-af72-49b8ee8137dc.\n",
      "23/11/13 10:03:16 INFO FileFormatWriter: Write Job d14c1b2a-47cf-42c6-af72-49b8ee8137dc committed. Elapsed time: 13 ms.\n",
      "23/11/13 10:03:16 INFO FileFormatWriter: Finished processing stats for write job d14c1b2a-47cf-42c6-af72-49b8ee8137dc.\n",
      "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/mromanel/Documents/repetitions-homeric-speeches/data/input/input_lemmatised.json\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1011)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:785)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
      "23/11/13 10:03:16 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "23/11/13 10:03:16 INFO SparkUI: Stopped Spark web UI at http://lettres-118-35.unil.ch:4040\n",
      "23/11/13 10:03:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/11/13 10:03:16 INFO MemoryStore: MemoryStore cleared\n",
      "23/11/13 10:03:16 INFO BlockManager: BlockManager stopped\n",
      "23/11/13 10:03:16 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/11/13 10:03:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/11/13 10:03:16 INFO SparkContext: Successfully stopped SparkContext\n",
      "23/11/13 10:03:16 INFO ShutdownHookManager: Shutdown hook called\n",
      "23/11/13 10:03:16 INFO ShutdownHookManager: Deleting directory /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-39da92ca-0316-4d8b-b87b-4e12162287e2\n",
      "23/11/13 10:03:16 INFO ShutdownHookManager: Deleting directory /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7\n"
     ]
    }
   ],
   "source": [
    "# run passim on lemmatised and filtered speeches\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 1 --min-match 1 -a 5 --max-repeat 100 -w 1 data/input/homeric_speeches_lemmatised.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Applications/spark-3.3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/mromanel/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/mromanel/.ivy2/jars\n",
      "com.github.scopt#scopt_2.12 added as a dependency\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f8a030d3-60e7-4b34-8369-12f5c637355f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.github.scopt#scopt_2.12;3.5.0 in central\n",
      "\tfound graphframes#graphframes;0.8.0-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 166ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.scopt#scopt_2.12;3.5.0 from central in [default]\n",
      "\tgraphframes#graphframes;0.8.0-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f8a030d3-60e7-4b34-8369-12f5c637355f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "23/11/13 10:16:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/13 10:16:07 INFO SparkContext: Running Spark version 3.3.2\n",
      "23/11/13 10:16:07 INFO ResourceUtils: ==============================================================\n",
      "23/11/13 10:16:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/11/13 10:16:07 INFO ResourceUtils: ==============================================================\n",
      "23/11/13 10:16:07 INFO SparkContext: Submitted application: passim.PassimApp\n",
      "23/11/13 10:16:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/11/13 10:16:07 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/11/13 10:16:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/11/13 10:16:07 INFO SecurityManager: Changing view acls to: mromanel\n",
      "23/11/13 10:16:07 INFO SecurityManager: Changing modify acls to: mromanel\n",
      "23/11/13 10:16:07 INFO SecurityManager: Changing view acls groups to: \n",
      "23/11/13 10:16:07 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/11/13 10:16:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(mromanel); groups with view permissions: Set(); users  with modify permissions: Set(mromanel); groups with modify permissions: Set()\n",
      "23/11/13 10:16:08 INFO Utils: Successfully started service 'sparkDriver' on port 50475.\n",
      "23/11/13 10:16:08 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/11/13 10:16:08 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/11/13 10:16:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/11/13 10:16:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/11/13 10:16:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/11/13 10:16:08 INFO DiskBlockManager: Created local directory at /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/blockmgr-3889cda6-3ed7-450d-87fa-9ab3aa3200ec\n",
      "23/11/13 10:16:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/11/13 10:16:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/11/13 10:16:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/11/13 10:16:08 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/com.github.scopt_scopt_2.12-3.5.0.jar at spark://lettres-118-35.unil.ch:50475/jars/com.github.scopt_scopt_2.12-3.5.0.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar at spark://lettres-118-35.unil.ch:50475/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://lettres-118-35.unil.ch:50475/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO SparkContext: Added JAR file:/Users/mromanel/Documents/passim-1.0.0/target/scala-2.12/passim_2.12-0.2.0.jar at spark://lettres-118-35.unil.ch:50475/jars/passim_2.12-0.2.0.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO Executor: Starting executor ID driver on host lettres-118-35.unil.ch\n",
      "23/11/13 10:16:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/11/13 10:16:08 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50475/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO TransportClientFactory: Successfully created connection to lettres-118-35.unil.ch/130.223.118.35:50475 after 27 ms (0 ms spent in bootstraps)\n",
      "23/11/13 10:16:08 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50475/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/fetchFileTemp6787787729224784616.tmp\n",
      "23/11/13 10:16:08 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar to class loader\n",
      "23/11/13 10:16:08 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50475/jars/com.github.scopt_scopt_2.12-3.5.0.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50475/jars/com.github.scopt_scopt_2.12-3.5.0.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/fetchFileTemp1892795630772641388.tmp\n",
      "23/11/13 10:16:08 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/com.github.scopt_scopt_2.12-3.5.0.jar to class loader\n",
      "23/11/13 10:16:08 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50475/jars/passim_2.12-0.2.0.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50475/jars/passim_2.12-0.2.0.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/fetchFileTemp9483203111750037121.tmp\n",
      "23/11/13 10:16:08 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/passim_2.12-0.2.0.jar to class loader\n",
      "23/11/13 10:16:08 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50475/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50475/jars/org.slf4j_slf4j-api-1.7.16.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/fetchFileTemp13245684161293278588.tmp\n",
      "23/11/13 10:16:08 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
      "23/11/13 10:16:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50477.\n",
      "23/11/13 10:16:08 INFO NettyBlockTransferService: Server created on lettres-118-35.unil.ch:50477\n",
      "23/11/13 10:16:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/11/13 10:16:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lettres-118-35.unil.ch, 50477, None)\n",
      "23/11/13 10:16:08 INFO BlockManagerMasterEndpoint: Registering block manager lettres-118-35.unil.ch:50477 with 434.4 MiB RAM, BlockManagerId(driver, lettres-118-35.unil.ch, 50477, None)\n",
      "23/11/13 10:16:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lettres-118-35.unil.ch, 50477, None)\n",
      "23/11/13 10:16:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, lettres-118-35.unil.ch, 50477, None)\n",
      "23/11/13 10:16:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/11/13 10:16:09 INFO SharedState: Warehouse path is 'file:/Users/mromanel/Documents/repetitions-homeric-speeches/spark-warehouse'.\n",
      "23/11/13 10:16:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/11/13 10:16:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/11/13 10:16:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/11/13 10:16:10 INFO CodeGenerator: Code generated in 135.387524 ms\n",
      "23/11/13 10:16:10 INFO SparkContext: Starting job: json at PassimApp.scala:1360\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Got job 0 (json at PassimApp.scala:1360) with 1 output partitions\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Final stage: ResultStage 0 (json at PassimApp.scala:1360)\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Submitting ResultStage 0 (CoalescedRDD[2] at json at PassimApp.scala:1360), which has no missing parents\n",
      "23/11/13 10:16:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 156.6 KiB, free 434.2 MiB)\n",
      "23/11/13 10:16:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 56.3 KiB, free 434.2 MiB)\n",
      "23/11/13 10:16:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on lettres-118-35.unil.ch:50477 (size: 56.3 KiB, free: 434.3 MiB)\n",
      "23/11/13 10:16:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "23/11/13 10:16:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (CoalescedRDD[2] at json at PassimApp.scala:1360) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/13 10:16:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/11/13 10:16:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (lettres-118-35.unil.ch, executor driver, partition 0, PROCESS_LOCAL, 5063 bytes) taskResourceAssignments Map()\n",
      "23/11/13 10:16:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/11/13 10:16:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/11/13 10:16:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/11/13 10:16:11 INFO FileOutputCommitter: Saved output of task 'attempt_202311131016103963232811565660281_0000_m_000000_0' to file:/Users/mromanel/Documents/repetitions-homeric-speeches/data/passim/exp0/conf/_temporary/0/task_202311131016103963232811565660281_0000_m_000000\n",
      "23/11/13 10:16:11 INFO SparkHadoopMapRedUtil: attempt_202311131016103963232811565660281_0000_m_000000_0: Committed. Elapsed time: 1 ms.\n",
      "23/11/13 10:16:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1452 bytes result sent to driver\n",
      "23/11/13 10:16:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 197 ms on lettres-118-35.unil.ch (executor driver) (1/1)\n",
      "23/11/13 10:16:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/11/13 10:16:11 INFO DAGScheduler: ResultStage 0 (json at PassimApp.scala:1360) finished in 0.651 s\n",
      "23/11/13 10:16:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/13 10:16:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/11/13 10:16:11 INFO DAGScheduler: Job 0 finished: json at PassimApp.scala:1360, took 0.681717 s\n",
      "23/11/13 10:16:11 INFO FileFormatWriter: Start to commit write Job 9914cf74-8c15-49fe-b51e-fafe1d2903c3.\n",
      "23/11/13 10:16:11 INFO FileFormatWriter: Write Job 9914cf74-8c15-49fe-b51e-fafe1d2903c3 committed. Elapsed time: 12 ms.\n",
      "23/11/13 10:16:11 INFO FileFormatWriter: Finished processing stats for write job 9914cf74-8c15-49fe-b51e-fafe1d2903c3.\n",
      "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/mromanel/Documents/repetitions-homeric-speeches/data/input/homeric_speeches.json\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1011)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:785)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
      "23/11/13 10:16:11 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "23/11/13 10:16:11 INFO SparkUI: Stopped Spark web UI at http://lettres-118-35.unil.ch:4040\n",
      "23/11/13 10:16:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/11/13 10:16:11 INFO MemoryStore: MemoryStore cleared\n",
      "23/11/13 10:16:11 INFO BlockManager: BlockManager stopped\n",
      "23/11/13 10:16:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/11/13 10:16:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/11/13 10:16:11 INFO SparkContext: Successfully stopped SparkContext\n",
      "23/11/13 10:16:11 INFO ShutdownHookManager: Shutdown hook called\n",
      "23/11/13 10:16:11 INFO ShutdownHookManager: Deleting directory /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-429d118c-8845-44a4-8c03-4e551c967b7e\n",
      "23/11/13 10:16:11 INFO ShutdownHookManager: Deleting directory /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b\n"
     ]
    }
   ],
   "source": [
    "# run passim on raw text speeches (no lemmatisation)\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 1 --min-match 1 -a 5 --max-repeat 100 -w 1 data/input/homeric_speeches_raw.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7993 text reuse clusters in data/passim/exp0/out.json/\n"
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "#tr_clusters = passim_output_to_dataframe(passim_json_output_path, tsv_path)\n",
    "tr_clusters = passim_output_to_dataframe(\n",
    "    passim_json_output_path, \n",
    "    tsv_path,\n",
    "    columns_to_keep=['cluster', 'id', 'label', 'dices_tags', 'dices_speech_id', 'text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>dices_tags</th>\n",
       "      <th>dices_speech_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Homer, Iliad 8.352-8.356</td>\n",
       "      <td>que</td>\n",
       "      <td>223</td>\n",
       "      <td>ὢ πόποι αἰγιόχοιο Διὸς τέκος οὐκέτι νῶϊ ὀλλυμέ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer, Iliad 22.450-22.459</td>\n",
       "      <td>del</td>\n",
       "      <td>605</td>\n",
       "      <td>δεῦτε δύω μοι ἕπεσθον, ἴδωμʼ ὅτινʼ ἔργα τέτυκτ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer, Iliad 8.358-8.380</td>\n",
       "      <td>del|des|lam</td>\n",
       "      <td>224</td>\n",
       "      <td>καὶ λίην οὗτός γε μένος θυμόν τʼ ὀλέσειε χερσὶ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer, Iliad 22.477-22.514</td>\n",
       "      <td>lam|ora</td>\n",
       "      <td>606</td>\n",
       "      <td>ἄρα γεινόμεθʼ αἴσῃ ἀμφότεροι, σὺ μὲν ἐν Τροίῃ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer, Iliad 8.399-8.408</td>\n",
       "      <td>com</td>\n",
       "      <td>225</td>\n",
       "      <td>βάσκʼ ἴθι Ἶρι ταχεῖα, πάλιν τρέπε μηδʼ ἔα ἄντη...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Homer, Iliad 23.6-23.11</td>\n",
       "      <td>del</td>\n",
       "      <td>608</td>\n",
       "      <td>Μυρμιδόνες ταχύπωλοι ἐμοὶ ἐρίηρες ἑταῖροι μὴ δ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Homer, Iliad 8.413-8.424</td>\n",
       "      <td>mes|que|war</td>\n",
       "      <td>226</td>\n",
       "      <td>πῇ μέματον; τί σφῶϊν ἐνὶ φρεσὶ μαίνεται ἦτορ; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Homer, Iliad 8.427-8.431</td>\n",
       "      <td>del</td>\n",
       "      <td>227</td>\n",
       "      <td>ὢ πόποι αἰγιόχοιο Διὸς τέκος, οὐκέτʼ ἔγωγε νῶϊ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Homer, Iliad 23.19-23.23</td>\n",
       "      <td>vow</td>\n",
       "      <td>609</td>\n",
       "      <td>Πάτροκλε καὶ εἰν Ἀΐδαο δόμοισι· πάντα γὰρ ἤδη ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Homer, Iliad 8.447-8.456</td>\n",
       "      <td>lau|que|tau</td>\n",
       "      <td>228</td>\n",
       "      <td>οὕτω τετίησθον Ἀθηναίη τε καὶ Ἥρη; οὐ μέν θην ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster  id                       label   dices_tags  dices_speech_id   \n",
       "0        0   0    Homer, Iliad 8.352-8.356          que              223  \\\n",
       "1        0   1  Homer, Iliad 22.450-22.459          del              605   \n",
       "2        0   2    Homer, Iliad 8.358-8.380  del|des|lam              224   \n",
       "3        0   3  Homer, Iliad 22.477-22.514      lam|ora              606   \n",
       "4        0   4    Homer, Iliad 8.399-8.408          com              225   \n",
       "5        0   6     Homer, Iliad 23.6-23.11          del              608   \n",
       "6        0   7    Homer, Iliad 8.413-8.424  mes|que|war              226   \n",
       "7        0   8    Homer, Iliad 8.427-8.431          del              227   \n",
       "8        0   9    Homer, Iliad 23.19-23.23          vow              609   \n",
       "9        0  10    Homer, Iliad 8.447-8.456  lau|que|tau              228   \n",
       "\n",
       "                                                text  \n",
       "0  ὢ πόποι αἰγιόχοιο Διὸς τέκος οὐκέτι νῶϊ ὀλλυμέ...  \n",
       "1  δεῦτε δύω μοι ἕπεσθον, ἴδωμʼ ὅτινʼ ἔργα τέτυκτ...  \n",
       "2  καὶ λίην οὗτός γε μένος θυμόν τʼ ὀλέσειε χερσὶ...  \n",
       "3  ἄρα γεινόμεθʼ αἴσῃ ἀμφότεροι, σὺ μὲν ἐν Τροίῃ ...  \n",
       "4  βάσκʼ ἴθι Ἶρι ταχεῖα, πάλιν τρέπε μηδʼ ἔα ἄντη...  \n",
       "5  Μυρμιδόνες ταχύπωλοι ἐμοὶ ἐρίηρες ἑταῖροι μὴ δ...  \n",
       "6  πῇ μέματον; τί σφῶϊν ἐνὶ φρεσὶ μαίνεται ἦτορ; ...  \n",
       "7  ὢ πόποι αἰγιόχοιο Διὸς τέκος, οὐκέτʼ ἔγωγε νῶϊ...  \n",
       "8  Πάτροκλε καὶ εἰν Ἀΐδαο δόμοισι· πάντα γὰρ ἤδη ...  \n",
       "9  οὕτω τετίησθον Ἀθηναίη τε καὶ Ἥρη; οὐ μέν θην ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_clusters.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 (exp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "output_path = 'data/passim/exp4/'\n",
    "tsv_path = 'data/output/passim_clusters_exp4.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Applications/spark-3.3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/mromanel/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/mromanel/.ivy2/jars\n",
      "com.github.scopt#scopt_2.12 added as a dependency\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7157a7f0-389d-40b2-a698-ef7976e1438c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.github.scopt#scopt_2.12;3.5.0 in central\n",
      "\tfound graphframes#graphframes;0.8.0-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 169ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.scopt#scopt_2.12;3.5.0 from central in [default]\n",
      "\tgraphframes#graphframes;0.8.0-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7157a7f0-389d-40b2-a698-ef7976e1438c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "23/11/13 10:03:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/13 10:03:12 INFO SparkContext: Running Spark version 3.3.2\n",
      "23/11/13 10:03:12 INFO ResourceUtils: ==============================================================\n",
      "23/11/13 10:03:12 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/11/13 10:03:12 INFO ResourceUtils: ==============================================================\n",
      "23/11/13 10:03:12 INFO SparkContext: Submitted application: passim.PassimApp\n",
      "23/11/13 10:03:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/11/13 10:03:12 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/11/13 10:03:12 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/11/13 10:03:12 INFO SecurityManager: Changing view acls to: mromanel\n",
      "23/11/13 10:03:12 INFO SecurityManager: Changing modify acls to: mromanel\n",
      "23/11/13 10:03:12 INFO SecurityManager: Changing view acls groups to: \n",
      "23/11/13 10:03:12 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/11/13 10:03:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(mromanel); groups with view permissions: Set(); users  with modify permissions: Set(mromanel); groups with modify permissions: Set()\n",
      "23/11/13 10:03:12 INFO Utils: Successfully started service 'sparkDriver' on port 50395.\n",
      "23/11/13 10:03:12 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/11/13 10:03:12 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/11/13 10:03:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/11/13 10:03:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/11/13 10:03:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/11/13 10:03:12 INFO DiskBlockManager: Created local directory at /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/blockmgr-5d7f0027-8f67-4bdb-b61d-9f0539cc374e\n",
      "23/11/13 10:03:12 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/11/13 10:03:12 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/11/13 10:03:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/11/13 10:03:12 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/com.github.scopt_scopt_2.12-3.5.0.jar at spark://lettres-118-35.unil.ch:50395/jars/com.github.scopt_scopt_2.12-3.5.0.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:12 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar at spark://lettres-118-35.unil.ch:50395/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:12 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://lettres-118-35.unil.ch:50395/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:12 INFO SparkContext: Added JAR file:/Users/mromanel/Documents/passim-1.0.0/target/scala-2.12/passim_2.12-0.2.0.jar at spark://lettres-118-35.unil.ch:50395/jars/passim_2.12-0.2.0.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO Executor: Starting executor ID driver on host lettres-118-35.unil.ch\n",
      "23/11/13 10:03:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/11/13 10:03:13 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50395/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO TransportClientFactory: Successfully created connection to lettres-118-35.unil.ch/130.223.118.35:50395 after 30 ms (0 ms spent in bootstraps)\n",
      "23/11/13 10:03:13 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50395/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/fetchFileTemp16810074473699614266.tmp\n",
      "23/11/13 10:03:13 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar to class loader\n",
      "23/11/13 10:03:13 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50395/jars/passim_2.12-0.2.0.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50395/jars/passim_2.12-0.2.0.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/fetchFileTemp4393892570678245168.tmp\n",
      "23/11/13 10:03:13 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/passim_2.12-0.2.0.jar to class loader\n",
      "23/11/13 10:03:13 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50395/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50395/jars/org.slf4j_slf4j-api-1.7.16.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/fetchFileTemp10488466867369647312.tmp\n",
      "23/11/13 10:03:13 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
      "23/11/13 10:03:13 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50395/jars/com.github.scopt_scopt_2.12-3.5.0.jar with timestamp 1699866192032\n",
      "23/11/13 10:03:13 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50395/jars/com.github.scopt_scopt_2.12-3.5.0.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/fetchFileTemp12587294255740282014.tmp\n",
      "23/11/13 10:03:13 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7/userFiles-c058a423-283e-4d5c-a417-2b34c8c4ac30/com.github.scopt_scopt_2.12-3.5.0.jar to class loader\n",
      "23/11/13 10:03:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50397.\n",
      "23/11/13 10:03:13 INFO NettyBlockTransferService: Server created on lettres-118-35.unil.ch:50397\n",
      "23/11/13 10:03:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/11/13 10:03:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lettres-118-35.unil.ch, 50397, None)\n",
      "23/11/13 10:03:13 INFO BlockManagerMasterEndpoint: Registering block manager lettres-118-35.unil.ch:50397 with 434.4 MiB RAM, BlockManagerId(driver, lettres-118-35.unil.ch, 50397, None)\n",
      "23/11/13 10:03:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lettres-118-35.unil.ch, 50397, None)\n",
      "23/11/13 10:03:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, lettres-118-35.unil.ch, 50397, None)\n",
      "23/11/13 10:03:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/11/13 10:03:13 INFO SharedState: Warehouse path is 'file:/Users/mromanel/Documents/repetitions-homeric-speeches/spark-warehouse'.\n",
      "23/11/13 10:03:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/11/13 10:03:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/11/13 10:03:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/11/13 10:03:15 INFO CodeGenerator: Code generated in 157.403739 ms\n",
      "23/11/13 10:03:15 INFO SparkContext: Starting job: json at PassimApp.scala:1360\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Got job 0 (json at PassimApp.scala:1360) with 1 output partitions\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Final stage: ResultStage 0 (json at PassimApp.scala:1360)\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/13 10:03:15 INFO DAGScheduler: Submitting ResultStage 0 (CoalescedRDD[2] at json at PassimApp.scala:1360), which has no missing parents\n",
      "23/11/13 10:03:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 156.6 KiB, free 434.2 MiB)\n",
      "23/11/13 10:03:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 56.3 KiB, free 434.2 MiB)\n",
      "23/11/13 10:03:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on lettres-118-35.unil.ch:50397 (size: 56.3 KiB, free: 434.3 MiB)\n",
      "23/11/13 10:03:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "23/11/13 10:03:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (CoalescedRDD[2] at json at PassimApp.scala:1360) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/13 10:03:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/11/13 10:03:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (lettres-118-35.unil.ch, executor driver, partition 0, PROCESS_LOCAL, 5063 bytes) taskResourceAssignments Map()\n",
      "23/11/13 10:03:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/11/13 10:03:16 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/11/13 10:03:16 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/11/13 10:03:16 INFO FileOutputCommitter: Saved output of task 'attempt_202311131003152978051997185313161_0000_m_000000_0' to file:/Users/mromanel/Documents/repetitions-homeric-speeches/data/passim/out_cluster/conf/_temporary/0/task_202311131003152978051997185313161_0000_m_000000\n",
      "23/11/13 10:03:16 INFO SparkHadoopMapRedUtil: attempt_202311131003152978051997185313161_0000_m_000000_0: Committed. Elapsed time: 1 ms.\n",
      "23/11/13 10:03:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1452 bytes result sent to driver\n",
      "23/11/13 10:03:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 215 ms on lettres-118-35.unil.ch (executor driver) (1/1)\n",
      "23/11/13 10:03:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/11/13 10:03:16 INFO DAGScheduler: ResultStage 0 (json at PassimApp.scala:1360) finished in 0.729 s\n",
      "23/11/13 10:03:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/13 10:03:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/11/13 10:03:16 INFO DAGScheduler: Job 0 finished: json at PassimApp.scala:1360, took 0.762568 s\n",
      "23/11/13 10:03:16 INFO FileFormatWriter: Start to commit write Job d14c1b2a-47cf-42c6-af72-49b8ee8137dc.\n",
      "23/11/13 10:03:16 INFO FileFormatWriter: Write Job d14c1b2a-47cf-42c6-af72-49b8ee8137dc committed. Elapsed time: 13 ms.\n",
      "23/11/13 10:03:16 INFO FileFormatWriter: Finished processing stats for write job d14c1b2a-47cf-42c6-af72-49b8ee8137dc.\n",
      "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/mromanel/Documents/repetitions-homeric-speeches/data/input/input_lemmatised.json\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1011)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:785)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
      "23/11/13 10:03:16 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "23/11/13 10:03:16 INFO SparkUI: Stopped Spark web UI at http://lettres-118-35.unil.ch:4040\n",
      "23/11/13 10:03:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/11/13 10:03:16 INFO MemoryStore: MemoryStore cleared\n",
      "23/11/13 10:03:16 INFO BlockManager: BlockManager stopped\n",
      "23/11/13 10:03:16 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/11/13 10:03:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/11/13 10:03:16 INFO SparkContext: Successfully stopped SparkContext\n",
      "23/11/13 10:03:16 INFO ShutdownHookManager: Shutdown hook called\n",
      "23/11/13 10:03:16 INFO ShutdownHookManager: Deleting directory /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-39da92ca-0316-4d8b-b87b-4e12162287e2\n",
      "23/11/13 10:03:16 INFO ShutdownHookManager: Deleting directory /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-1d831913-521b-40d6-aed3-80ef6195b8a7\n"
     ]
    }
   ],
   "source": [
    "# run passim on lemmatised and filtered speeches\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 2 --min-match 2 --max-repeat 100 -a 10 data/input/homeric_speeches_lemmatised.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1909 text reuse clusters in data/passim/exp4/out.json/\n"
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "tr_clusters = passim_output_to_dataframe(passim_json_output_path, tsv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Applications/spark-3.3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/mromanel/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/mromanel/.ivy2/jars\n",
      "com.github.scopt#scopt_2.12 added as a dependency\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f8a030d3-60e7-4b34-8369-12f5c637355f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.github.scopt#scopt_2.12;3.5.0 in central\n",
      "\tfound graphframes#graphframes;0.8.0-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 166ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.scopt#scopt_2.12;3.5.0 from central in [default]\n",
      "\tgraphframes#graphframes;0.8.0-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f8a030d3-60e7-4b34-8369-12f5c637355f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "23/11/13 10:16:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/13 10:16:07 INFO SparkContext: Running Spark version 3.3.2\n",
      "23/11/13 10:16:07 INFO ResourceUtils: ==============================================================\n",
      "23/11/13 10:16:07 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/11/13 10:16:07 INFO ResourceUtils: ==============================================================\n",
      "23/11/13 10:16:07 INFO SparkContext: Submitted application: passim.PassimApp\n",
      "23/11/13 10:16:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/11/13 10:16:07 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/11/13 10:16:07 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/11/13 10:16:07 INFO SecurityManager: Changing view acls to: mromanel\n",
      "23/11/13 10:16:07 INFO SecurityManager: Changing modify acls to: mromanel\n",
      "23/11/13 10:16:07 INFO SecurityManager: Changing view acls groups to: \n",
      "23/11/13 10:16:07 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/11/13 10:16:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(mromanel); groups with view permissions: Set(); users  with modify permissions: Set(mromanel); groups with modify permissions: Set()\n",
      "23/11/13 10:16:08 INFO Utils: Successfully started service 'sparkDriver' on port 50475.\n",
      "23/11/13 10:16:08 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/11/13 10:16:08 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/11/13 10:16:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/11/13 10:16:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/11/13 10:16:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/11/13 10:16:08 INFO DiskBlockManager: Created local directory at /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/blockmgr-3889cda6-3ed7-450d-87fa-9ab3aa3200ec\n",
      "23/11/13 10:16:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "23/11/13 10:16:08 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/11/13 10:16:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "23/11/13 10:16:08 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/com.github.scopt_scopt_2.12-3.5.0.jar at spark://lettres-118-35.unil.ch:50475/jars/com.github.scopt_scopt_2.12-3.5.0.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar at spark://lettres-118-35.unil.ch:50475/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO SparkContext: Added JAR file:///Users/mromanel/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://lettres-118-35.unil.ch:50475/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO SparkContext: Added JAR file:/Users/mromanel/Documents/passim-1.0.0/target/scala-2.12/passim_2.12-0.2.0.jar at spark://lettres-118-35.unil.ch:50475/jars/passim_2.12-0.2.0.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO Executor: Starting executor ID driver on host lettres-118-35.unil.ch\n",
      "23/11/13 10:16:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/11/13 10:16:08 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50475/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO TransportClientFactory: Successfully created connection to lettres-118-35.unil.ch/130.223.118.35:50475 after 27 ms (0 ms spent in bootstraps)\n",
      "23/11/13 10:16:08 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50475/jars/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/fetchFileTemp6787787729224784616.tmp\n",
      "23/11/13 10:16:08 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/graphframes_graphframes-0.8.0-spark3.0-s_2.12.jar to class loader\n",
      "23/11/13 10:16:08 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50475/jars/com.github.scopt_scopt_2.12-3.5.0.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50475/jars/com.github.scopt_scopt_2.12-3.5.0.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/fetchFileTemp1892795630772641388.tmp\n",
      "23/11/13 10:16:08 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/com.github.scopt_scopt_2.12-3.5.0.jar to class loader\n",
      "23/11/13 10:16:08 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50475/jars/passim_2.12-0.2.0.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50475/jars/passim_2.12-0.2.0.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/fetchFileTemp9483203111750037121.tmp\n",
      "23/11/13 10:16:08 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/passim_2.12-0.2.0.jar to class loader\n",
      "23/11/13 10:16:08 INFO Executor: Fetching spark://lettres-118-35.unil.ch:50475/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1699866967864\n",
      "23/11/13 10:16:08 INFO Utils: Fetching spark://lettres-118-35.unil.ch:50475/jars/org.slf4j_slf4j-api-1.7.16.jar to /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/fetchFileTemp13245684161293278588.tmp\n",
      "23/11/13 10:16:08 INFO Executor: Adding file:/private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b/userFiles-c2c71b92-7387-460b-bfc1-e5ff689407f1/org.slf4j_slf4j-api-1.7.16.jar to class loader\n",
      "23/11/13 10:16:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50477.\n",
      "23/11/13 10:16:08 INFO NettyBlockTransferService: Server created on lettres-118-35.unil.ch:50477\n",
      "23/11/13 10:16:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/11/13 10:16:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lettres-118-35.unil.ch, 50477, None)\n",
      "23/11/13 10:16:08 INFO BlockManagerMasterEndpoint: Registering block manager lettres-118-35.unil.ch:50477 with 434.4 MiB RAM, BlockManagerId(driver, lettres-118-35.unil.ch, 50477, None)\n",
      "23/11/13 10:16:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lettres-118-35.unil.ch, 50477, None)\n",
      "23/11/13 10:16:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, lettres-118-35.unil.ch, 50477, None)\n",
      "23/11/13 10:16:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/11/13 10:16:09 INFO SharedState: Warehouse path is 'file:/Users/mromanel/Documents/repetitions-homeric-speeches/spark-warehouse'.\n",
      "23/11/13 10:16:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "23/11/13 10:16:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/11/13 10:16:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/11/13 10:16:10 INFO CodeGenerator: Code generated in 135.387524 ms\n",
      "23/11/13 10:16:10 INFO SparkContext: Starting job: json at PassimApp.scala:1360\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Got job 0 (json at PassimApp.scala:1360) with 1 output partitions\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Final stage: ResultStage 0 (json at PassimApp.scala:1360)\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/13 10:16:10 INFO DAGScheduler: Submitting ResultStage 0 (CoalescedRDD[2] at json at PassimApp.scala:1360), which has no missing parents\n",
      "23/11/13 10:16:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 156.6 KiB, free 434.2 MiB)\n",
      "23/11/13 10:16:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 56.3 KiB, free 434.2 MiB)\n",
      "23/11/13 10:16:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on lettres-118-35.unil.ch:50477 (size: 56.3 KiB, free: 434.3 MiB)\n",
      "23/11/13 10:16:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "23/11/13 10:16:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (CoalescedRDD[2] at json at PassimApp.scala:1360) (first 15 tasks are for partitions Vector(0))\n",
      "23/11/13 10:16:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/11/13 10:16:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (lettres-118-35.unil.ch, executor driver, partition 0, PROCESS_LOCAL, 5063 bytes) taskResourceAssignments Map()\n",
      "23/11/13 10:16:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/11/13 10:16:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/11/13 10:16:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/11/13 10:16:11 INFO FileOutputCommitter: Saved output of task 'attempt_202311131016103963232811565660281_0000_m_000000_0' to file:/Users/mromanel/Documents/repetitions-homeric-speeches/data/passim/exp0/conf/_temporary/0/task_202311131016103963232811565660281_0000_m_000000\n",
      "23/11/13 10:16:11 INFO SparkHadoopMapRedUtil: attempt_202311131016103963232811565660281_0000_m_000000_0: Committed. Elapsed time: 1 ms.\n",
      "23/11/13 10:16:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1452 bytes result sent to driver\n",
      "23/11/13 10:16:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 197 ms on lettres-118-35.unil.ch (executor driver) (1/1)\n",
      "23/11/13 10:16:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/11/13 10:16:11 INFO DAGScheduler: ResultStage 0 (json at PassimApp.scala:1360) finished in 0.651 s\n",
      "23/11/13 10:16:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/13 10:16:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/11/13 10:16:11 INFO DAGScheduler: Job 0 finished: json at PassimApp.scala:1360, took 0.681717 s\n",
      "23/11/13 10:16:11 INFO FileFormatWriter: Start to commit write Job 9914cf74-8c15-49fe-b51e-fafe1d2903c3.\n",
      "23/11/13 10:16:11 INFO FileFormatWriter: Write Job 9914cf74-8c15-49fe-b51e-fafe1d2903c3 committed. Elapsed time: 12 ms.\n",
      "23/11/13 10:16:11 INFO FileFormatWriter: Finished processing stats for write job 9914cf74-8c15-49fe-b51e-fafe1d2903c3.\n",
      "Exception in thread \"main\" org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/mromanel/Documents/repetitions-homeric-speeches/data/input/homeric_speeches.json\n",
      "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:1011)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:785)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
      "23/11/13 10:16:11 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "23/11/13 10:16:11 INFO SparkUI: Stopped Spark web UI at http://lettres-118-35.unil.ch:4040\n",
      "23/11/13 10:16:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "23/11/13 10:16:11 INFO MemoryStore: MemoryStore cleared\n",
      "23/11/13 10:16:11 INFO BlockManager: BlockManager stopped\n",
      "23/11/13 10:16:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "23/11/13 10:16:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "23/11/13 10:16:11 INFO SparkContext: Successfully stopped SparkContext\n",
      "23/11/13 10:16:11 INFO ShutdownHookManager: Shutdown hook called\n",
      "23/11/13 10:16:11 INFO ShutdownHookManager: Deleting directory /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-429d118c-8845-44a4-8c03-4e551c967b7e\n",
      "23/11/13 10:16:11 INFO ShutdownHookManager: Deleting directory /private/var/folders/g_/f12m6prx6_54vv77tghqx5xw0000gq/T/spark-9a186878-af47-488a-99d1-27cfe287ce9b\n"
     ]
    }
   ],
   "source": [
    "# run passim on raw text speeches (no lemmatisation)\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 2 --min-match 2 --max-repeat 100 -a 10 data/input/homeric_speeches_raw.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2078 text reuse clusters in data/passim/exp4/out.json/\n"
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "tr_clusters = passim_output_to_dataframe(\n",
    "    passim_json_output_path, \n",
    "    tsv_path,\n",
    "    columns_to_keep=['cluster', 'id', 'label', 'dices_tags', 'dices_speech_id', 'text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>dices_tags</th>\n",
       "      <th>dices_speech_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Homer, Iliad 8.352-8.356</td>\n",
       "      <td>que</td>\n",
       "      <td>223</td>\n",
       "      <td>ὢ πόποι αἰγιόχοιο Διὸς τέκος οὐκέτι νῶϊ ὀλλυμέ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Homer, Iliad 22.450-22.459</td>\n",
       "      <td>del</td>\n",
       "      <td>605</td>\n",
       "      <td>δεῦτε δύω μοι ἕπεσθον, ἴδωμʼ ὅτινʼ ἔργα τέτυκτ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer, Iliad 8.358-8.380</td>\n",
       "      <td>del|des|lam</td>\n",
       "      <td>224</td>\n",
       "      <td>καὶ λίην οὗτός γε μένος θυμόν τʼ ὀλέσειε χερσὶ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer, Iliad 22.477-22.514</td>\n",
       "      <td>lam|ora</td>\n",
       "      <td>606</td>\n",
       "      <td>ἄρα γεινόμεθʼ αἴσῃ ἀμφότεροι, σὺ μὲν ἐν Τροίῃ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer, Iliad 8.399-8.408</td>\n",
       "      <td>com</td>\n",
       "      <td>225</td>\n",
       "      <td>βάσκʼ ἴθι Ἶρι ταχεῖα, πάλιν τρέπε μηδʼ ἔα ἄντη...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Homer, Iliad 23.6-23.11</td>\n",
       "      <td>del</td>\n",
       "      <td>608</td>\n",
       "      <td>Μυρμιδόνες ταχύπωλοι ἐμοὶ ἐρίηρες ἑταῖροι μὴ δ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Homer, Iliad 8.413-8.424</td>\n",
       "      <td>mes|que|war</td>\n",
       "      <td>226</td>\n",
       "      <td>πῇ μέματον; τί σφῶϊν ἐνὶ φρεσὶ μαίνεται ἦτορ; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Homer, Iliad 8.427-8.431</td>\n",
       "      <td>del</td>\n",
       "      <td>227</td>\n",
       "      <td>ὢ πόποι αἰγιόχοιο Διὸς τέκος, οὐκέτʼ ἔγωγε νῶϊ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Homer, Iliad 23.19-23.23</td>\n",
       "      <td>vow</td>\n",
       "      <td>609</td>\n",
       "      <td>Πάτροκλε καὶ εἰν Ἀΐδαο δόμοισι· πάντα γὰρ ἤδη ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Homer, Iliad 8.447-8.456</td>\n",
       "      <td>lau|que|tau</td>\n",
       "      <td>228</td>\n",
       "      <td>οὕτω τετίησθον Ἀθηναίη τε καὶ Ἥρη; οὐ μέν θην ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster  id                       label   dices_tags  dices_speech_id   \n",
       "0        0   0    Homer, Iliad 8.352-8.356          que              223  \\\n",
       "1        0   1  Homer, Iliad 22.450-22.459          del              605   \n",
       "2        0   2    Homer, Iliad 8.358-8.380  del|des|lam              224   \n",
       "3        0   3  Homer, Iliad 22.477-22.514      lam|ora              606   \n",
       "4        0   4    Homer, Iliad 8.399-8.408          com              225   \n",
       "5        0   6     Homer, Iliad 23.6-23.11          del              608   \n",
       "6        0   7    Homer, Iliad 8.413-8.424  mes|que|war              226   \n",
       "7        0   8    Homer, Iliad 8.427-8.431          del              227   \n",
       "8        0   9    Homer, Iliad 23.19-23.23          vow              609   \n",
       "9        0  10    Homer, Iliad 8.447-8.456  lau|que|tau              228   \n",
       "\n",
       "                                                text  \n",
       "0  ὢ πόποι αἰγιόχοιο Διὸς τέκος οὐκέτι νῶϊ ὀλλυμέ...  \n",
       "1  δεῦτε δύω μοι ἕπεσθον, ἴδωμʼ ὅτινʼ ἔργα τέτυκτ...  \n",
       "2  καὶ λίην οὗτός γε μένος θυμόν τʼ ὀλέσειε χερσὶ...  \n",
       "3  ἄρα γεινόμεθʼ αἴσῃ ἀμφότεροι, σὺ μὲν ἐν Τροίῃ ...  \n",
       "4  βάσκʼ ἴθι Ἶρι ταχεῖα, πάλιν τρέπε μηδʼ ἔα ἄντη...  \n",
       "5  Μυρμιδόνες ταχύπωλοι ἐμοὶ ἐρίηρες ἑταῖροι μὴ δ...  \n",
       "6  πῇ μέματον; τί σφῶϊν ἐνὶ φρεσὶ μαίνεται ἦτορ; ...  \n",
       "7  ὢ πόποι αἰγιόχοιο Διὸς τέκος, οὐκέτʼ ἔγωγε νῶϊ...  \n",
       "8  Πάτροκλε καὶ εἰν Ἀΐδαο δόμοισι· πάντα γὰρ ἤδη ...  \n",
       "9  οὕτω τετίησθον Ἀθηναίη τε καὶ Ἥρη; οὐ μέν θην ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_clusters.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5 (exp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "output_path = 'data/passim/exp5/'\n",
    "tsv_path = 'data/output/passim_clusters_exp5.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run passim on lemmatised and filtered speeches\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 3 --min-match 1 --max-repeat 100 -a 10 data/input/homeric_speeches_lemmatised.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 350 text reuse clusters in data/passim/exp5/out.json/\n"
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "tr_clusters = passim_output_to_dataframe(passim_json_output_path, tsv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run passim on raw text speeches (no lemmatisation)\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 3 --min-match 1 --max-repeat 100 -a 10 data/input/homeric_speeches_raw.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 431 text reuse clusters in data/passim/exp5/out.json/\n"
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "tr_clusters = passim_output_to_dataframe(\n",
    "    passim_json_output_path, \n",
    "    tsv_path,\n",
    "    columns_to_keep=['cluster', 'id', 'label', 'dices_tags', 'dices_speech_id', 'text']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6 (exp6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "output_path = '../data/passim/exp6/'\n",
    "tsv_path = '../data/output/passim_clusters_exp6.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run passim on lemmatised and filtered speeches\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 4 --min-match 1 --max-repeat 100 -a 10 data/input/homeric_speeches_lemmatised.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 431 text reuse clusters in ../data/passim/exp6/out.json/\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['raw_text'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/mromanel/Documents/repetitions-homeric-speeches/notebooks/Experiments.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mromanel/Documents/repetitions-homeric-speeches/notebooks/Experiments.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m passim_json_output_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_path, \u001b[39m'\u001b[39m\u001b[39mout.json/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mromanel/Documents/repetitions-homeric-speeches/notebooks/Experiments.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tr_clusters \u001b[39m=\u001b[39m passim_output_to_dataframe(passim_json_output_path, tsv_path)\n",
      "File \u001b[0;32m~/Documents/repetitions-homeric-speeches/notebooks/../lib/utils.py:19\u001b[0m, in \u001b[0;36mpassim_output_to_dataframe\u001b[0;34m(output_path, dataframe_path, columns_to_keep)\u001b[0m\n\u001b[1;32m     17\u001b[0m tr_clusters \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(read_jsonl(output_path))\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThere are \u001b[39m\u001b[39m{\u001b[39;00mtr_clusters\u001b[39m.\u001b[39mcluster\u001b[39m.\u001b[39munique()\u001b[39m.\u001b[39msize\u001b[39m}\u001b[39;00m\u001b[39m text reuse clusters in \u001b[39m\u001b[39m{\u001b[39;00moutput_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m output_df \u001b[39m=\u001b[39m tr_clusters[columns_to_keep]\n\u001b[1;32m     20\u001b[0m output_df\u001b[39m.\u001b[39mto_csv(dataframe_path)\n\u001b[1;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m output_df\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/dices-repetitions/lib/python3.10/site-packages/pandas/core/frame.py:3766\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3764\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3765\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3766\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3768\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3769\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/dices-repetitions/lib/python3.10/site-packages/pandas/core/indexes/base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5873\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5876\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5878\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5879\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5880\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/dices-repetitions/lib/python3.10/site-packages/pandas/core/indexes/base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5937\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 5938\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['raw_text'] not in index\""
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "tr_clusters = passim_output_to_dataframe(passim_json_output_path, tsv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run passim on raw text speeches (no lemmatisation)\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 4 --min-match 1 --max-repeat 100 -a 10 data/input/homeric_speeches_raw.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 341 text reuse clusters in ../data/passim/exp6/out.json/\n"
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "tr_clusters = passim_output_to_dataframe(\n",
    "    passim_json_output_path, \n",
    "    tsv_path,\n",
    "    columns_to_keep=['cluster', 'id', 'label', 'dices_tags', 'dices_speech_id', 'text', 'speaker', 'addressee']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7 (exp7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "output_path = 'data/passim/exp7/'\n",
    "tsv_path = 'data/output/passim_clusters_exp7.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run passim on lemmatised and filtered speeches\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 3 --min-match 2 --max-repeat 100 -a 10 data/input/homeric_speeches_lemmatised.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 350 text reuse clusters in data/passim/exp7/out.json/\n"
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "tr_clusters = passim_output_to_dataframe(passim_json_output_path, tsv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run passim on raw text speeches (no lemmatisation)\n",
    "! /Users/mromanel/Documents/passim-1.0.0/bin/passim -n 3 --min-match 2 --max-repeat 100 -a 10 data/input/homeric_speeches_raw.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 431 text reuse clusters in data/passim/exp7/out.json/\n"
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "tr_clusters = passim_output_to_dataframe(\n",
    "    passim_json_output_path, \n",
    "    tsv_path,\n",
    "    columns_to_keep=['cluster', 'id', 'label', 'dices_tags', 'dices_speech_id', 'text']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments (`seriatim`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 8 (exp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n",
      ":: loading settings :: url = jar:file:/Users/matteo/.pyenv/versions/3.10.0/envs/homeric-repetitions/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/matteo/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/matteo/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-62255efc-621b-44b1-85d2-5d33a3e384f9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.0-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in spark-list\n",
      ":: resolution report :: resolve 183ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.0-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from spark-list in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-62255efc-621b-44b1-85d2-5d33a3e384f9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n",
      "usage: submit-seriatim.py [-h] [-i ID] [-t TEXT] [--locs LOCS] [--pages PAGES]\n",
      "                          [-l N] [-u N] [-m N] [-n N] [--floating-ngrams]\n",
      "                          [--complete-lines] [-g N] [--max-offset N]\n",
      "                          [--beam N] [--pcopy p] [-a N] [--src-overlap p]\n",
      "                          [--dst-overlap p] [--fields FIELDS [FIELDS ...]]\n",
      "                          [-f FILTERPAIRS] [--all-pairs] [--pairwise]\n",
      "                          [--docwise] [--linewise] [--to-pairs] [--to-extents]\n",
      "                          [--link-model LINK_MODEL]\n",
      "                          [--link-features LINK_FEATURES]\n",
      "                          [--log-level {ERROR,WARN,INFO,DEBUG}]\n",
      "                          [--input-format INPUT_FORMAT]\n",
      "                          [--output-format OUTPUT_FORMAT]\n",
      "                          <path> <path>\n",
      "\n",
      "Passim Alignment\n",
      "\n",
      "positional arguments:\n",
      "  <path>                input data\n",
      "  <path>                output\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -i ID, --id ID        Field for unique document IDs (default: id)\n",
      "  -t TEXT, --text TEXT  Field for document text (default: text)\n",
      "  --locs LOCS           Field for citable loci (default: locs)\n",
      "  --pages PAGES         Field for page location information (default: pages)\n",
      "  -l N, --minDF N       Lower limit on document frequency (default: 2)\n",
      "  -u N, --maxDF N       Upper limit on document frequency (default: 100)\n",
      "  -m N, --min-match N   Minimum number of n-gram matches between documents\n",
      "                        (default: 5)\n",
      "  -n N, --n N           n-gram order (default: 25)\n",
      "  --floating-ngrams     Allow n-grams to float from word boundaries (default:\n",
      "                        False)\n",
      "  --complete-lines      Break target alignments at line breaks (default:\n",
      "                        False)\n",
      "  -g N, --gap N         Minimum size of gap that separates passages (default:\n",
      "                        600)\n",
      "  --max-offset N        Maximum offset in global alignment [deprecated]\n",
      "                        (default: 20)\n",
      "  --beam N              Beam search width (default: 20)\n",
      "  --pcopy p             Probability of copying a character (default: 0.8)\n",
      "  -a N, --min-align N   Minimum length of alignment (default: 50)\n",
      "  --src-overlap p       Source overlap proportion (default: 0.9)\n",
      "  --dst-overlap p       Destination overlap proportion (default: 0.5)\n",
      "  --fields FIELDS [FIELDS ...]\n",
      "                        List of fields to index (default: [])\n",
      "  -f FILTERPAIRS, --filterpairs FILTERPAIRS\n",
      "                        SQL constraint on posting pairs (default: uid < uid2)\n",
      "  --all-pairs           Compute alignments for all pairs. (default: False)\n",
      "  --pairwise            Output pairwise alignments (default: False)\n",
      "  --docwise             Output docwise alignments (default: False)\n",
      "  --linewise            Output linewise alignments (default: False)\n",
      "  --to-pairs            Output pairs and stop (default: False)\n",
      "  --to-extents          Output extents and stop (default: False)\n",
      "  --link-model LINK_MODEL\n",
      "                        Link model in R format (default: None)\n",
      "  --link-features LINK_FEATURES\n",
      "                        Link model features as SQL SELECT (default: None)\n",
      "  --log-level {ERROR,WARN,INFO,DEBUG}\n",
      "                        spark log level (default: WARN)\n",
      "  --input-format INPUT_FORMAT\n",
      "                        Input format (default: json)\n",
      "  --output-format OUTPUT_FORMAT\n",
      "                        Output format (default: json)\n"
     ]
    }
   ],
   "source": [
    "!seriatim --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "output_path = 'data/seriatim/exp8/'\n",
    "tsv_path = 'data/output/seriatim_clusters_exp8.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: data/seriatim/exp8/: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# cleaning passim's output folder\n",
    "!rm -r {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run passim on raw text speeches (no lemmatisation)\n",
    "!seriatim -n 10 --min-match 1 -a 10 --minDF 2 --maxDF 100 data/input/homeric_speeches_raw.json {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 431 text reuse clusters in data/passim/exp7/out.json/\n"
     ]
    }
   ],
   "source": [
    "passim_json_output_path = os.path.join(output_path, 'out.json/')\n",
    "tr_clusters = passim_output_to_dataframe(\n",
    "    passim_json_output_path, \n",
    "    tsv_path,\n",
    "    columns_to_keep=['cluster', 'id', 'label', 'dices_tags', 'dices_speech_id', 'text']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dices-repetitions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
